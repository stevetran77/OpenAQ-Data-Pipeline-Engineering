# Airflow environment for OpenAQ pipeline (dev)
# IMPORTANT: Copy this file to airflow.env and fill in your actual values.
# DO NOT commit airflow.env with real secrets; add it to .gitignore.

AIRFLOW_HOME=/opt/airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags

# Postgres metadata DB (service name 'postgres' from docker-compose)
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow_reddit

# Fernet key for encrypting connections/variables
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# IMPORTANT: Replace with a secure key for production
AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY_HERE

# Webserver settings
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
AIRFLOW__WEBSERVER__RBAC=True

# Logging
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# Preconfigure AWS connection in Airflow (dev)
# Format: aws://AWS_ACCESS_KEY:AWS_SECRET_KEY@?region_name=REGION
# IMPORTANT: Use IAM temporary credentials or rotate keys regularly for production
AIRFLOW_CONN_AWS_DEFAULT=aws://YOUR_AWS_ACCESS_KEY:YOUR_AWS_SECRET_KEY@?region_name=ap-southeast-1
